{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Volumes/CAS9/Aeiwz/Documents/Thesis/Analyse/Dataset/U_noesy_pqn.csv')\n",
    "meta = pd.read_excel('/Volumes/CAS9/Aeiwz/Documents/Thesis/dataset/Sample sheet for Thesis copy.xlsx', sheet_name='Urine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppm = pd.read_csv('/Volumes/CAS9/Aeiwz/Documents/Thesis/dataset/ppm_U_noesy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 14:]\n",
    "X.columns = list(np.ravel(ppm))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([meta, X], axis=1)\n",
    "df2.to_csv('/Volumes/CAS9/Aeiwz/Documents/Thesis/Analyse/Dataset/U_noesy_pqn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Volumes/CAS9/Aeiwz/Documents/Thesis/Analyse/Report/OPLSDA_P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_path[-1] == '/':\n",
    "    #remove the last /\n",
    "    data_path = data_path[:-1]\n",
    "    \n",
    "else:\n",
    "    data_path = data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genpage import gen_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_page(data_path=data_path).get_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot = glob(pathname= data_path+'/element/hist_plot/*.html')\n",
    "Lingress_ = glob(pathname= data_path+'/element/Lingress/*.html')\n",
    "loading_plot = glob(pathname= data_path+'/element/loading_plot/*.html')\n",
    "s_plot = glob(pathname= data_path+'/element/s_plot/*.html')\n",
    "score_plot = glob(pathname= data_path+'/element/score_plot/*.html')\n",
    "\n",
    "files = pd.DataFrame({'hist_plot': hist_plot, 'Lingress_': Lingress_, 'loading_plot': loading_plot, 's_plot': s_plot, 'score_plot': score_plot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace value in dataframe with ..\n",
    "files = files.replace(to_replace=data_path, value='..', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files['names'] = files['hist_plot'].str.split('/').str[-1].str.split('.').str[0]\n",
    "files['names'] = files['names'].str.replace('Permutation_scores_','')\n",
    "files['names'] = files['names'].str.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files['names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/KOI_Xal.csv')\n",
    "ppm = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/KOI_ppm.csv')\n",
    "meta = pd.read_excel('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/RunOrder_CK 2.xlsx', sheet_name='Data (Original)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = list(np.ravel(ppm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([meta, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/dataset_KOI_Xal.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PCA analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/dataset_KOI_Xal.csv')\n",
    "#print(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingress.unipair import Unipair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['Technique'] == \"QC\"].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = Unipair.indexing(meta = df, column_name=\"Cardioplegia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the required python packages including \n",
    "# the custom Chemometric Model objects\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyChemometrics.ChemometricsPCA import ChemometricsPCA\n",
    "from pyChemometrics.ChemometricsScaler import ChemometricsScaler\n",
    "\n",
    "# Use to obtain same values as in the text\n",
    "np.random.seed(350)\n",
    "\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import scale\n",
    "from pca_ellipse import confidence_ellipse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/dataset_KOI_Xal.csv')\n",
    "#Drop QC samples\n",
    "df = df.drop(df[df['Technique'] == 'QC'].index)\n",
    "\n",
    "Group=\"Cardioplegia\"\n",
    "\n",
    "test_index = Unipair.indexing(meta = df, column_name=Group)\n",
    "\n",
    "\n",
    "\n",
    "c_list = test_index\n",
    "\n",
    "\n",
    "#Make directory\n",
    "# path folder\n",
    "report_path = '/Volumes/CAS9/Aeiwz/Project/KOI_CC/Report'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('{}/PCA_result'.format(report_path), exist_ok=True)\n",
    "os.makedirs('{}/PCA_result/HTML'.format(report_path), exist_ok=True)\n",
    "os.makedirs('{}/PCA_result/PNG'.format(report_path), exist_ok=True)\n",
    "os.makedirs('{}/PCA_result/Scores'.format(report_path), exist_ok=True)\n",
    "os.makedirs('{}/PCA_result/Loading'.format(report_path), exist_ok=True)\n",
    "os.makedirs('{}/PCA_result/R2'.format(report_path), exist_ok=True)\n",
    "os.makedirs('{}/PCA_result/Trajectory'.format(report_path), exist_ok=True)\n",
    "\n",
    "\n",
    "for i in range(len(c_list)):\n",
    "    \n",
    "    \n",
    "\n",
    "    plot_name = i\n",
    "    \n",
    "    plot_name = str(plot_name)\n",
    "\n",
    "    # path folder\n",
    "    PCA_result_path = '{}/PCA_result'.format(report_path)\n",
    "    HTML_save = '{}/HTML'.format(PCA_result_path)\n",
    "    PNG_save = '{}/PNG'.format(PCA_result_path)\n",
    "    Scores_save = '{}/Scores'.format(PCA_result_path)\n",
    "    Loading_save = '{}/Loading'.format(PCA_result_path)\n",
    "    R2_save = '{}/R2'.format(PCA_result_path)\n",
    "    Trajectory_save = '{}/Trajectory'.format(PCA_result_path)\n",
    "\n",
    "    # Import the datasets from the /data directory\n",
    "    # X for the NMR spectra and Y for the 2 outcome variables\n",
    "    test_gr = df.loc[c_list[i]]\n",
    "\n",
    "    X = test_gr.iloc[:, 112:]\n",
    "    #fill nan with 0\n",
    "    X = X.fillna(0)\n",
    "    meta = test_gr.iloc[:, :112]\n",
    "    Y = test_gr[Group]\n",
    "    Y1 = pd.Categorical(Y).codes\n",
    "    ppm = list(np.ravel(X.columns).astype(float))\n",
    "    # Use pandas Categorical type to generate the dummy enconding of the Y vector (0 and 1) \n",
    "\n",
    "   \n",
    "\n",
    "    import time\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    T1 = time.time()\n",
    "\n",
    "\n",
    "     # Select the scaling options: \n",
    "    # Here we are generating 3 scaling objects to explore the effect of scaling in PCA:\n",
    "\n",
    "    # Unit-Variance (UV) scaling:\n",
    "    \n",
    "    scale__ = 'UV'\n",
    "    scale_power_ = 1\n",
    "\n",
    "    # Mean Centering (MC):\n",
    "    #scaling_object_mc = ChemometricsScaler(scale_power=0)\n",
    "\n",
    "    # Pareto scaling (Par):\n",
    "    # scaling_object_par = ChemometricsScaler(scale_power=0.5)\n",
    "\n",
    "    \n",
    "    model_scaler = ChemometricsScaler(scale_power=scale_power_)\n",
    "    model_scaler.fit(X)\n",
    "    model_X = model_scaler.transform(X)\n",
    "\n",
    "    pca_model = decomposition.PCA(n_components=2)\n",
    "    pca_model.fit(model_X)\n",
    "\n",
    "    scores_ = pca_model.transform(model_X)\n",
    "    df_scores_ = pd.DataFrame(scores_, columns=['PC1', 'PC2'])\n",
    "    df_scores_.index = test_gr.index\n",
    "\n",
    "    df2_scores_ = pd.concat([df_scores_, Y], axis=1)\n",
    "\n",
    "    #save PCA score to csv\n",
    "    df2_scores_.to_csv(Scores_save+'/PCA_scores_'+ plot_name +'.csv')\n",
    "\n",
    "    loadings_ = pca_model.components_.T\n",
    "    df_loadings_ = pd.DataFrame(loadings_, columns=['PC1', 'PC2'], index=np.ravel(ppm))\n",
    "    df_loadings_.to_csv(Loading_save + '/Loading_scores ' + plot_name + '.csv')\n",
    "\n",
    "    explained_variance_ = pca_model.explained_variance_ratio_\n",
    "    explained_variance_\n",
    "\n",
    "    explained_variance_ = np.insert(explained_variance_, 0, 0)\n",
    "\n",
    "    cumulative_variance_ = np.cumsum(np.round(explained_variance_, decimals=3))\n",
    "\n",
    "    pc_df_ = pd.DataFrame(['','PC1', 'PC2'], columns=['PC'])\n",
    "    explained_variance_df_ = pd.DataFrame(explained_variance_, columns=['Explained Variance'])\n",
    "    cumulative_variance_df_ = pd.DataFrame(cumulative_variance_, columns=['Cumulative Variance'])\n",
    "\n",
    "    df_explained_variance_ = pd.concat([pc_df_, explained_variance_df_, cumulative_variance_df_], axis=1)\n",
    "    df_explained_variance_.to_csv(R2_save + '/R2 ' + plot_name + '.csv')\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=357)\n",
    "    X_test = model_scaler.transform(X_test)\n",
    "    X_test_pca = pca_model.transform(X_test)\n",
    "\n",
    "    # Inverse transform the test set from the PCA space\n",
    "    X_test_reconstructed = pca_model.inverse_transform(X_test_pca)\n",
    "\n",
    "\n",
    "    # Calculate Q2 score for the test set\n",
    "    q2_test = r2_score(X_test, X_test_reconstructed)\n",
    "           \n",
    "\n",
    "    # Plot\n",
    "\n",
    "    # https://plotly.com/python/bar-charts/\n",
    "\n",
    "    fig = px.bar(df_explained_variance_, \n",
    "                x='PC', y='Explained Variance',\n",
    "                text='Explained Variance',\n",
    "                width=800, height=600,\n",
    "                title='Explained Variance ({} scaling)'.format(scale__))\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'y':0.9,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        font=dict(size=15))\n",
    "    fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')\n",
    "    #fig.show()\n",
    "    fig.write_image(PNG_save + \"/Explained Variance \" + plot_name + \".png\")\n",
    "    fig.write_html(HTML_save + \"/Explained Variance \" + plot_name + \".html\")\n",
    "\n",
    "    # https://plotly.com/python/creating-and-updating-figures/\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_explained_variance_['PC'],\n",
    "            y=df_explained_variance_['Cumulative Variance'],\n",
    "            marker=dict(size=15, color=\"LightSeaGreen\"),\n",
    "            name='R<sup>2</sup>X (Cum)'\n",
    "        ))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_explained_variance_['PC'],\n",
    "            y=df_explained_variance_['Explained Variance'],\n",
    "            marker=dict(color=\"RoyalBlue\"),\n",
    "            name='R<sup>2</sup>X',\n",
    "            text=np.round(df_explained_variance_['Explained Variance'], decimals=3)\n",
    "        ))\n",
    "    fig.update_layout(width=800, height=600,\n",
    "                    title='Explained Variance and Cumulative Variance ' + plot_name)\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'y':0.9,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'})\n",
    "\n",
    "    #fig.show()\n",
    "    fig.write_image(PNG_save + \"/Explained Variance + Cumulative Variance \" + plot_name + \".png\")\n",
    "    fig.write_html(HTML_save + \"/Explained Variance + Cumulative Variance \" + plot_name + \".html\")\n",
    "\n",
    "\n",
    "    colour_dict = {\n",
    "                    \"Off pump CABG (OPCAB)\": \"#E91E63\",        \n",
    "                    \"On pump CABG (ONCAB)\": \"#FF9800\",\n",
    "                    \"custodiol\": \"#FFEB3B\",       \n",
    "                    \"del Nido\": \"#9C27B0\",\n",
    "                    \"st. thomas\": \"#03A9F4\",\n",
    "                    \"No med\": \"#4CAF50\",        \n",
    "                    \"7\": \"#B30000\",\n",
    "                    \"8\": \"#3F51B5\"\n",
    "                    }\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # PCA plot\n",
    "    pca_label = df2_scores_.index\n",
    "\n",
    "\n",
    "    fig = px.scatter(df2_scores_, x='PC1', y='PC2', symbol=meta['Time point'],\n",
    "                     \n",
    "                     symbol_map={'Pre-op': 'circle',\n",
    "                            'Op': 'diamond',\n",
    "                            'Post-op1': 'square',\n",
    "                            'Post-op2': 'triangle-up',\n",
    "                            'Post-op3': 'triangle-down'},\n",
    "                     \n",
    "                    color=Group,\n",
    "                    color_discrete_map=colour_dict, \n",
    "                    title='<b>PCA Scores Plot ({} Scaling)<b>'.format(scale__), \n",
    "                    height=900, width=1300,\n",
    "                    labels={\"PC1\": \"PC1 R<sup>2</sup>X: {} %\".format(np.round(df_explained_variance_.iloc[1,1]*100, decimals=2)),\n",
    "                            \"PC2\": \"PC2 R<sup>2</sup>X: {} %\".format(np.round(df_explained_variance_.iloc[2,1]*100, decimals=2))})\n",
    "\n",
    "    #fig.add_annotation(yref = 'paper', y = -1.06, xref = 'paper', x=1.06 , text='Q2' +' = {}'.format(np.round(df_explained_variance_.iloc[2,2], decimals=2)))\n",
    "    #fig.update_annotations(font = {\n",
    "    #    'size': 20}, showarrow=False)\n",
    "\n",
    "    #set data point fill alpha with boarder in each color\n",
    "    fig.update_traces(marker=dict(size=35, opacity=0.7, line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                            #x=x_loc,\n",
    "                            x=1.0,\n",
    "                            y=0.05,\n",
    "                            showarrow=False,\n",
    "                            text='<b>R<sup>2</sup>X (Cum): {}%<b>'.format(np.round(df_explained_variance_.iloc[2,2]*100, decimals=2)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                            #x=x_loc,\n",
    "                            x=1.0,\n",
    "                            y=0.01,\n",
    "                            showarrow=False,\n",
    "                            text='<b>Q<sup>2</sup>X (Cum): {}%<b>'.format(np.round(q2_test*100, decimals=2)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "    fig.update_traces(marker=dict(size=20))\n",
    "    #fig.update_traces(textposition='top center') #Text label position\n",
    "\n",
    "    #fig.update_traces(marker=dict(size=12, color=Y1_color, marker=Y2_marker))\n",
    "    fig.add_shape(type='path',\n",
    "                path=confidence_ellipse(df2_scores_['PC1'], df2_scores_['PC2']))\n",
    "\n",
    "\n",
    "\n",
    "    fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'y':1,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        font=dict(size=20))\n",
    "    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "\n",
    "    #fig.show()\n",
    "    fig.write_image(PNG_save + \"/PCA \" + plot_name + \".png\")\n",
    "    fig.write_html(HTML_save + \"/PCA \" + plot_name + \".html\")\n",
    "\n",
    "\n",
    "# Loading plot\n",
    "    loadings_label = df_loadings_.index\n",
    "\n",
    "\n",
    "    fig = px.line(df_loadings_, x=loadings_label, y=['PC1', 'PC2'],\n",
    "                    height=600, width=1800,\n",
    "                    title='Loadings ' + plot_name\n",
    "                    )\n",
    "\n",
    "    fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_layout(title={'y':0.95,\n",
    "                            'x':0.5,\n",
    "                            'xanchor': 'center',\n",
    "                            'yanchor': 'top'},\n",
    "                    font=dict(size=20))\n",
    "    \n",
    "    fig.update_layout(scene={'xaxis': {'autorange': 'reversed'}})\n",
    "            \n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "    fig.update_layout(xaxis_title=\"𝛿<sub>H</sub> in ppm\")\n",
    "    #fig.show()\n",
    "\n",
    "    fig.write_image(PNG_save + \"/Loading \" + plot_name + \".png\")\n",
    "    fig.write_html(HTML_save + \"/Loading \" + plot_name + \".html\")\n",
    "    \n",
    "    \n",
    "    #Time trajectory\n",
    "    traject_df = df2_scores_.copy()\n",
    "    traject_df['Time point'] = meta['Time point']\n",
    "    med_df = traject_df.groupby(['Time point', Group]).median()\n",
    "    err_df = traject_df.groupby(['Time point', Group]).sem()\n",
    "    \n",
    "    med_df = med_df.reset_index()\n",
    "    err_df = err_df.reset_index()\n",
    "    \n",
    "    \n",
    "    med_df.sort_values(by=['Time point'], inplace=True, key=lambda x: x.map({\"Pre-op\": 0, \"Op\": 1, \"Post-op1\": 2, \"Post-op2\": 3, \"Post-op3\": 4}))\n",
    "    med_df.sort_values(by=[Group], inplace=True)\n",
    "    med_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    err_df.sort_values(by=['Time point'], inplace=True, key=lambda x: x.map({\"Pre-op\": 0, \"Op\": 1, \"Post-op1\": 2, \"Post-op2\": 3, \"Post-op3\": 4}))\n",
    "    err_df.sort_values(by=[Group], inplace=True)\n",
    "    err_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    list_label = med_df[Group].unique()\n",
    "    #line 1\n",
    "    if list_label[0] == 'Off pump CABG (OPCAB)': \n",
    "        colour1 = '#E91E63'\n",
    "    elif list_label[0] == 'On pump CABG (ONCAB)':\n",
    "        colour1 = '#FF9800'\n",
    "    elif list_label[0] == 'custodiol':\n",
    "        colour1 = '#FFEB3B'\n",
    "    elif list_label[0] == 'del Nido':\n",
    "        colour1 = '#9C27B0'\n",
    "    elif list_label[0] == 'st. thomas':\n",
    "        colour1 = '#03A9F4'\n",
    "    elif list_label[0] == 'No med':\n",
    "        colour1 = '#4CAF50'\n",
    "    \n",
    "    #line 2\n",
    "    if list_label[1] == 'Off pump CABG (OPCAB)':\n",
    "        colour2 = '#E91E63'\n",
    "    elif list_label[1] == 'On pump CABG (ONCAB)':\n",
    "        colour2 = '#FF9800'\n",
    "    elif list_label[1] == 'custodiol':\n",
    "        colour2 = '#FFEB3B'\n",
    "    elif list_label[1] == 'del Nido':\n",
    "        colour2 = '#9C27B0'\n",
    "    elif list_label[1] == 'st. thomas':\n",
    "        colour2 = '#03A9F4'\n",
    "    elif list_label[1] == 'No med':\n",
    "        colour2 = '#4CAF50'\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    fig = px.line(med_df, x='PC1', y='PC2', line_group='Time point', error_x=err_df[\"PC1\"], error_y=err_df[\"PC2\"],\n",
    "                    color=Group, \n",
    "                    symbol='Time point',\n",
    "                    color_discrete_map=colour_dict,\n",
    "                    symbol_map={'Pre-op': 'circle',\n",
    "                            'Op': 'diamond',\n",
    "                            'Post-op1': 'square',\n",
    "                            'Post-op2': 'triangle-up',\n",
    "                            'Post-op3': 'triangle-down'}, \n",
    "                    title='<b>Principle component analysis (Pareto scaling)<b>', \n",
    "                    height=900, width=1300,\n",
    "                    labels={\"PC1\": \"PC1 R<sup>2</sup>X: {} %\".format(24.3),\n",
    "                            \"PC2\": \"PC2 R<sup>2</sup>X: {} %\".format(15.0)})\n",
    "\n",
    "\n",
    "    # create a new trace for the connecting line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=med_df.loc[0:4, \"PC1\"], # x-coordinates of the line\n",
    "        y=med_df.loc[0:4, \"PC2\"], # y-coordinates of the line\n",
    "        mode='lines', # specify the trace type as lines\n",
    "        line=dict(color=colour1, width=2), # set the color and width of the line\n",
    "        showlegend=False # hide the trace from the legend\n",
    "    ))\n",
    "\n",
    "\n",
    "    # create a new trace for the connecting line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=med_df.loc[5:9, \"PC1\"], # x-coordinates of the line\n",
    "        y=med_df.loc[5:9, \"PC2\"], # y-coordinates of the line\n",
    "        mode='lines', # specify the trace type as lines\n",
    "        line=dict(color=colour2, width=2), # set the color and width of the line\n",
    "        showlegend=False # hide the trace from the legend\n",
    "    ))\n",
    "    '''\n",
    "    # create a new trace for the connecting line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_score_mean.iloc[4:8, 0], # x-coordinates of the line\n",
    "        y=df_score_mean.iloc[4:8, 2], # y-coordinates of the line\n",
    "        mode='lines', # specify the trace type as lines\n",
    "        line=dict(color='#84CC56', width=2), # set the color and width of the line\n",
    "        showlegend=False # hide the trace from the legend\n",
    "    ))\n",
    "\n",
    "\n",
    "    # create a new trace for the connecting line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_score_mean.iloc[8:12, 0], # x-coordinates of the line\n",
    "        y=df_score_mean.iloc[8:12, 2], # y-coordinates of the line\n",
    "        mode='lines', # specify the trace type as lines\n",
    "        line=dict(color='#CA83CC', width=2), # set the color and width of the line\n",
    "        showlegend=False # hide the trace from the legend\n",
    "    ))\n",
    "\n",
    "    # create a new trace for the connecting line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_score_mean.iloc[8:12, 0], # x-coordinates of the line\n",
    "        y=df_score_mean.iloc[8:12, 2], # y-coordinates of the line\n",
    "        mode='lines', # specify the trace type as lines\n",
    "        line=dict(color='#6AE022', width=2), # set the color and width of the line\n",
    "        showlegend=False # hide the trace from the legend\n",
    "    ))\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                            #x=x_loc,\n",
    "                            x=1.0,\n",
    "                            y=0.05,\n",
    "                            showarrow=False,\n",
    "                            text='<b>R<sup>2</sup>X (Cum): {}%<b>'.format(np.round(df_explained_variance_.iloc[2,2]*100, decimals=2)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                            #x=x_loc,\n",
    "                            x=1.0,\n",
    "                            y=0.01,\n",
    "                            showarrow=False,\n",
    "                            text='<b>Q<sup>2</sup>X (Cum): {}%<b>'.format(np.round(q2_test*100, decimals=2)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "\n",
    "    fig.update_traces(marker=dict(size=20))\n",
    "    #fig.update_traces(textposition='top center') #Text label position\n",
    "\n",
    "    #fig.update_traces(marker=dict(size=12, color=Y1_color, marker=Y2_marker))\n",
    "    fig.add_shape(type='path',\n",
    "                path=confidence_ellipse(med_df['PC1'],med_df['PC2']))\n",
    "\n",
    "\n",
    "\n",
    "    #update axis as scitifics\n",
    "    fig.update_xaxes(tickformat=\".1e\")\n",
    "    fig.update_yaxes(tickformat=\".1e\")\n",
    "\n",
    "\n",
    "\n",
    "    fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        font=dict(size=20))\n",
    "    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "    #fig.show()\n",
    "    \n",
    "    fig.write_image(\"{}/Save_PCA_trajectory_\".format(Trajectory_save) + plot_name + \".png\")\n",
    "    fig.write_html(\"{}/Save_PCA_trajectory_\".format(Trajectory_save) + plot_name + \".html\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    T2 = time.time()\n",
    "\n",
    "    print('{} Done /n Time taken: {} seconds'.format(plot_name, T2-T1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Code/dataset_1.csv')\n",
    "df2 = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Code/dataset_2.csv')\n",
    "df3 = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Code/dataset_3.csv')\n",
    "df4 = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Code/dataset_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1['Technique'].unique()), len(df2['Cardioplegia'].unique()), len(df3['Cardioplegia'].unique()), len(df4['Technique'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df2_scores_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5['Time'] = meta['Time point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df = df_5.groupby(['Time', 'Cardioplegia']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df\n",
    "std_df = df_5.groupby(['Time', 'Cardioplegia']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by \"Pre-op\", \"Op\", \"Post-op 1\", \"Post-op 2\", \"Post-op 3\", \"Portal vein\"\n",
    "med_df.sort_values(by=['Time'], inplace=True, key=lambda x: x.map({\"Pre-op\": 0, \"Op\": 1, \"Post-op1\": 2, \"Post-op2\": 3, \"Post-op3\": 4}))\n",
    "med_df.sort_values(by=[Group], inplace=True)\n",
    "med_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time trajectory\n",
    "traject_df = df2_scores_.copy()\n",
    "traject_df['Time point'] = meta['Time point']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df = traject_df.groupby(['Time point', Group]).median()\n",
    "err_df = traject_df.groupby(['Time point', Group]).sem()\n",
    "\n",
    "med_df = med_df.reset_index()\n",
    "err_df = err_df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "med_df.sort_values(by=['Time point'], inplace=True, key=lambda x: x.map({\"Pre-op\": 0, \"Op\": 1, \"Post-op1\": 2, \"Post-op2\": 3, \"Post-op3\": 4}))\n",
    "med_df.sort_values(by=[Group], inplace=True)\n",
    "med_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "err_df.sort_values(by=['Time point'], inplace=True, key=lambda x: x.map({\"Pre-op\": 0, \"Op\": 1, \"Post-op1\": 2, \"Post-op2\": 3, \"Post-op3\": 4}))\n",
    "err_df.sort_values(by=[Group], inplace=True)\n",
    "err_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/dataset_KOI_Xal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tech = dfx.copy()\n",
    "idx1 = df_tech[df_tech['AFONOP'] == 1].index\n",
    "idx2 = df_tech[df_tech['AFONOP'] == 2].index\n",
    "idx_ = idx1.append(idx2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tech = df_tech.loc[idx_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# All\n",
    "\n",
    "# Import the required python packages including \n",
    "# the custom Chemometric Model objects\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyChemometrics.ChemometricsPCA import ChemometricsPCA\n",
    "from pyChemometrics.ChemometricsScaler import ChemometricsScaler\n",
    "\n",
    "# Use to obtain same values as in the text\n",
    "np.random.seed(350)\n",
    "\n",
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import scale\n",
    "from pca_ellipse import confidence_ellipse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from lingress.unipair import Unipair\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Code/dataset_1.csv')\n",
    "#Drop QC samples\n",
    "df = df.drop(df[df['Technique'] == 'QC'].index)\n",
    "\n",
    "Group=\"Technique\"\n",
    "plot_name = \"All_1\"\n",
    "\n",
    "\n",
    "#Make directory\n",
    "# path folder\n",
    "report_path = '/Volumes/CAS9/Aeiwz/Project/KOI_CC/Report'\n",
    "project = 'PCA_report_all_1'\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('{}/{}'.format(report_path, project), exist_ok=True)\n",
    "os.makedirs('{}/{}/HTML'.format(report_path, project), exist_ok=True)\n",
    "os.makedirs('{}/{}/PNG'.format(report_path, project), exist_ok=True)\n",
    "os.makedirs('{}/{}/Scores'.format(report_path, project), exist_ok=True)\n",
    "os.makedirs('{}/{}/Loading'.format(report_path, project), exist_ok=True)\n",
    "os.makedirs('{}/{}/R2'.format(report_path, project), exist_ok=True)\n",
    "os.makedirs('{}/{}/Trajectory'.format(report_path, project), exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# path folder\n",
    "PCA_result_path = '{}/{}'.format(report_path, project)\n",
    "HTML_save = '{}/HTML'.format(PCA_result_path)\n",
    "PNG_save = '{}/PNG'.format(PCA_result_path)\n",
    "Scores_save = '{}/Scores'.format(PCA_result_path)\n",
    "Loading_save = '{}/Loading'.format(PCA_result_path)\n",
    "R2_save = '{}/R2'.format(PCA_result_path)\n",
    "Trajectory_save = '{}/Trajectory'.format(PCA_result_path)\n",
    "\n",
    "# Import the datasets from the /data directory\n",
    "# X for the NMR spectra and Y for the 2 outcome variables\n",
    "test_gr = df\n",
    "\n",
    "X = test_gr.iloc[:, 113:]\n",
    "#fill nan with 0\n",
    "X = X.fillna(0)\n",
    "meta = test_gr.iloc[:, :113]\n",
    "Y = test_gr[Group]\n",
    "Y1 = pd.Categorical(Y).codes\n",
    "ppm = list(np.ravel(X.columns).astype(float))\n",
    "# Use pandas Categorical type to generate the dummy enconding of the Y vector (0 and 1) \n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "T1 = time.time()\n",
    "\n",
    "\n",
    "    # Select the scaling options: \n",
    "# Here we are generating 3 scaling objects to explore the effect of scaling in PCA:\n",
    "\n",
    "# Unit-Variance (UV) scaling:\n",
    "\n",
    "scale__ = 'Par'\n",
    "scale_power_ = 0.5\n",
    "\n",
    "# Mean Centering (MC):\n",
    "#scaling_object_mc = ChemometricsScaler(scale_power=0)\n",
    "\n",
    "# Pareto scaling (Par):\n",
    "# scaling_object_par = ChemometricsScaler(scale_power=0.5)\n",
    "\n",
    "\n",
    "model_scaler = ChemometricsScaler(scale_power=scale_power_)\n",
    "model_scaler.fit(X)\n",
    "model_X = model_scaler.transform(X)\n",
    "\n",
    "pca_model = decomposition.PCA(n_components=2)\n",
    "pca_model.fit(model_X)\n",
    "\n",
    "scores_ = pca_model.transform(model_X)\n",
    "df_scores_ = pd.DataFrame(scores_, columns=['PC1', 'PC2'])\n",
    "df_scores_.index = test_gr.index\n",
    "\n",
    "df2_scores_ = pd.concat([df_scores_, Y], axis=1)\n",
    "\n",
    "#save PCA score to csv\n",
    "df2_scores_.to_csv(Scores_save+'/PCA_scores_'+ plot_name +'.csv')\n",
    "\n",
    "loadings_ = pca_model.components_.T\n",
    "df_loadings_ = pd.DataFrame(loadings_, columns=['PC1', 'PC2'], index=np.ravel(ppm))\n",
    "df_loadings_.to_csv(Loading_save + '/Loading_scores ' + plot_name + '.csv')\n",
    "\n",
    "explained_variance_ = pca_model.explained_variance_ratio_\n",
    "explained_variance_\n",
    "\n",
    "explained_variance_ = np.insert(explained_variance_, 0, 0)\n",
    "\n",
    "cumulative_variance_ = np.cumsum(np.round(explained_variance_, decimals=3))\n",
    "\n",
    "pc_df_ = pd.DataFrame(['','PC1', 'PC2'], columns=['PC'])\n",
    "explained_variance_df_ = pd.DataFrame(explained_variance_, columns=['Explained Variance'])\n",
    "cumulative_variance_df_ = pd.DataFrame(cumulative_variance_, columns=['Cumulative Variance'])\n",
    "\n",
    "df_explained_variance_ = pd.concat([pc_df_, explained_variance_df_, cumulative_variance_df_], axis=1)\n",
    "df_explained_variance_.to_csv(R2_save + '/R2 ' + plot_name + '.csv')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=357)\n",
    "X_test = model_scaler.transform(X_test)\n",
    "X_test_pca = pca_model.transform(X_test)\n",
    "\n",
    "# Inverse transform the test set from the PCA space\n",
    "X_test_reconstructed = pca_model.inverse_transform(X_test_pca)\n",
    "\n",
    "\n",
    "# Calculate Q2 score for the test set\n",
    "q2_test = r2_score(X_test, X_test_reconstructed)\n",
    "        \n",
    "\n",
    "# Plot\n",
    "\n",
    "# https://plotly.com/python/bar-charts/\n",
    "\n",
    "fig = px.bar(df_explained_variance_, \n",
    "            x='PC', y='Explained Variance',\n",
    "            text='Explained Variance',\n",
    "            width=800, height=600,\n",
    "            title='Explained Variance ({} scaling)'.format(scale__))\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    font=dict(size=15))\n",
    "fig.update_traces(texttemplate='%{text:.3f}', textposition='outside')\n",
    "#fig.show()\n",
    "fig.write_image(PNG_save + \"/Explained Variance \" + plot_name + \".png\")\n",
    "fig.write_html(HTML_save + \"/Explained Variance \" + plot_name + \".html\")\n",
    "\n",
    "# https://plotly.com/python/creating-and-updating-figures/\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_explained_variance_['PC'],\n",
    "        y=df_explained_variance_['Cumulative Variance'],\n",
    "        marker=dict(size=15, color=\"LightSeaGreen\"),\n",
    "        name='R<sup>2</sup>X (Cum)'\n",
    "    ))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=df_explained_variance_['PC'],\n",
    "        y=df_explained_variance_['Explained Variance'],\n",
    "        marker=dict(color=\"RoyalBlue\"),\n",
    "        name='R<sup>2</sup>X',\n",
    "        text=np.round(df_explained_variance_['Explained Variance'], decimals=3)\n",
    "    ))\n",
    "fig.update_layout(width=800, height=600,\n",
    "                title='Explained Variance and Cumulative Variance ' + plot_name)\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "\n",
    "#fig.show()\n",
    "fig.write_image(PNG_save + \"/Explained Variance + Cumulative Variance \" + plot_name + \".png\")\n",
    "fig.write_html(HTML_save + \"/Explained Variance + Cumulative Variance \" + plot_name + \".html\")\n",
    "\n",
    "\n",
    "colour_dict = {\n",
    "                \"Off pump CABG (OPCAB)\": \"#E91E63\",        \n",
    "                \"On pump CABG (ONCAB)\": \"#FF9800\",\n",
    "                \"custodiol\": \"#FFEB3B\",       \n",
    "                \"del Nido\": \"#9C27B0\",\n",
    "                \"st. thomas\": \"#03A9F4\",\n",
    "                \"No med\": \"#4CAF50\",        \n",
    "                \"7\": \"#B30000\",\n",
    "                \"8\": \"#3F51B5\"\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PCA plot\n",
    "pca_label = df2_scores_.index\n",
    "\n",
    "\n",
    "fig = px.scatter(df2_scores_, x='PC1', y='PC2', symbol=meta['Time point'],\n",
    "                    \n",
    "                    symbol_map={'Pre-op': 'circle',\n",
    "                        'Op': 'diamond',\n",
    "                        'Post-op1': 'square',\n",
    "                        'Post-op2': 'triangle-up',\n",
    "                        'Post-op3': 'triangle-down'},\n",
    "                    \n",
    "                color=Group,\n",
    "                color_discrete_map=colour_dict, \n",
    "                title='<b>PCA Scores Plot ({} Scaling)<b>'.format(scale__), \n",
    "                height=900, width=1300,\n",
    "                labels={\"PC1\": \"PC1 R<sup>2</sup>X: {} %\".format(np.round(df_explained_variance_.iloc[1,1]*100, decimals=2)),\n",
    "                        \"PC2\": \"PC2 R<sup>2</sup>X: {} %\".format(np.round(df_explained_variance_.iloc[2,1]*100, decimals=2))})\n",
    "\n",
    "#fig.add_annotation(yref = 'paper', y = -1.06, xref = 'paper', x=1.06 , text='Q2' +' = {}'.format(np.round(df_explained_variance_.iloc[2,2], decimals=2)))\n",
    "#fig.update_annotations(font = {\n",
    "#    'size': 20}, showarrow=False)\n",
    "\n",
    "#set data point fill alpha with boarder in each color\n",
    "fig.update_traces(marker=dict(size=35, opacity=0.7, line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                        #x=x_loc,\n",
    "                        x=1.0,\n",
    "                        y=0.05,\n",
    "                        showarrow=False,\n",
    "                        text='<b>R<sup>2</sup>X (Cum): {}%<b>'.format(np.round(df_explained_variance_.iloc[2,2]*100, decimals=2)),\n",
    "                        textangle=0,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"),\n",
    "                        # set alignment of text to left side of entry\n",
    "                        align=\"left\")\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                        #x=x_loc,\n",
    "                        x=1.0,\n",
    "                        y=0.01,\n",
    "                        showarrow=False,\n",
    "                        text='<b>Q<sup>2</sup>X (Cum): {}%<b>'.format(np.round(q2_test*100, decimals=2)),\n",
    "                        textangle=0,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"),\n",
    "                        # set alignment of text to left side of entry\n",
    "                        align=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "fig.update_traces(marker=dict(size=20))\n",
    "#fig.update_traces(textposition='top center') #Text label position\n",
    "\n",
    "#fig.update_traces(marker=dict(size=12, color=Y1_color, marker=Y2_marker))\n",
    "fig.add_shape(type='path',\n",
    "            path=confidence_ellipse(df2_scores_['PC1'], df2_scores_['PC2']))\n",
    "\n",
    "\n",
    "\n",
    "fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'y':1,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    font=dict(size=20))\n",
    "fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "\n",
    "#fig.show()\n",
    "fig.write_image(PNG_save + \"/PCA \" + plot_name + \".png\")\n",
    "fig.write_html(HTML_save + \"/PCA \" + plot_name + \".html\")\n",
    "\n",
    "\n",
    "# Loading plot\n",
    "loadings_label = df_loadings_.index\n",
    "\n",
    "\n",
    "fig = px.line(df_loadings_, x=loadings_label, y=['PC1', 'PC2'],\n",
    "                height=600, width=1800,\n",
    "                title='Loadings ' + plot_name\n",
    "                )\n",
    "\n",
    "fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "fig.update_layout(title={'y':0.95,\n",
    "                        'x':0.5,\n",
    "                        'xanchor': 'center',\n",
    "                        'yanchor': 'top'},\n",
    "                font=dict(size=20))\n",
    "\n",
    "fig.update_layout(scene={'xaxis': {'autorange': 'reversed'}})\n",
    "        \n",
    "fig.update_traces(marker=dict(size=1))\n",
    "fig.update_layout(xaxis_title=\"𝛿<sub>H</sub> in ppm\")\n",
    "#fig.show()\n",
    "\n",
    "fig.write_image(PNG_save + \"/Loading \" + plot_name + \".png\")\n",
    "fig.write_html(HTML_save + \"/Loading \" + plot_name + \".html\")\n",
    "\n",
    "\n",
    "#Time trajectory\n",
    "traject_df = df2_scores_.copy()\n",
    "traject_df['Time point'] = meta['Time point']\n",
    "med_df = traject_df.groupby(['Time point', Group]).median()\n",
    "err_df = traject_df.groupby(['Time point', Group]).sem()\n",
    "\n",
    "med_df = med_df.reset_index()\n",
    "err_df = err_df.reset_index()\n",
    "\n",
    "\n",
    "med_df.sort_values(by=['Time point'], inplace=True, key=lambda x: x.map({\"Pre-op\": 0, \"Op\": 1, \"Post-op1\": 2, \"Post-op2\": 3, \"Post-op3\": 4}))\n",
    "med_df.sort_values(by=[Group], inplace=True)\n",
    "med_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "err_df.sort_values(by=['Time point'], inplace=True, key=lambda x: x.map({\"Pre-op\": 0, \"Op\": 1, \"Post-op1\": 2, \"Post-op2\": 3, \"Post-op3\": 4}))\n",
    "err_df.sort_values(by=[Group], inplace=True)\n",
    "err_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "list_label = med_df[Group].unique()\n",
    "#line 1\n",
    "if list_label[0] == 'Off pump CABG (OPCAB)': \n",
    "    colour1 = '#E91E63'\n",
    "elif list_label[0] == 'On pump CABG (ONCAB)':\n",
    "    colour1 = '#FF9800'\n",
    "elif list_label[0] == 'custodiol':\n",
    "    colour1 = '#FFEB3B'\n",
    "elif list_label[0] == 'del Nido':\n",
    "    colour1 = '#9C27B0'\n",
    "elif list_label[0] == 'st. thomas':\n",
    "    colour1 = '#03A9F4'\n",
    "elif list_label[0] == 'No med':\n",
    "    colour1 = '#4CAF50'\n",
    "\n",
    "#line 2\n",
    "if list_label[1] == 'Off pump CABG (OPCAB)':\n",
    "    colour2 = '#E91E63'\n",
    "elif list_label[1] == 'On pump CABG (ONCAB)':\n",
    "    colour2 = '#FF9800'\n",
    "elif list_label[1] == 'custodiol':\n",
    "    colour2 = '#FFEB3B'\n",
    "elif list_label[1] == 'del Nido':\n",
    "    colour2 = '#9C27B0'\n",
    "elif list_label[1] == 'st. thomas':\n",
    "    colour2 = '#03A9F4'\n",
    "elif list_label[1] == 'No med':\n",
    "    colour2 = '#4CAF50'\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "fig = px.line(med_df, x='PC1', y='PC2', line_group='Time point', error_x=err_df[\"PC1\"], error_y=err_df[\"PC2\"],\n",
    "                color=Group, \n",
    "                symbol='Time point',\n",
    "                color_discrete_map=colour_dict,\n",
    "                symbol_map={'Pre-op': 'circle',\n",
    "                        'Op': 'diamond',\n",
    "                        'Post-op1': 'square',\n",
    "                        'Post-op2': 'triangle-up',\n",
    "                        'Post-op3': 'triangle-down'}, \n",
    "                title='<b>Principle component analysis (Pareto scaling)<b>', \n",
    "                height=900, width=1300,\n",
    "                labels={\"PC1\": \"PC1 R<sup>2</sup>X: {} %\".format(24.3),\n",
    "                        \"PC2\": \"PC2 R<sup>2</sup>X: {} %\".format(15.0)})\n",
    "\n",
    "\n",
    "# create a new trace for the connecting line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=med_df.loc[0:4, \"PC1\"], # x-coordinates of the line\n",
    "    y=med_df.loc[0:4, \"PC2\"], # y-coordinates of the line\n",
    "    mode='lines', # specify the trace type as lines\n",
    "    line=dict(color=colour1, width=2), # set the color and width of the line\n",
    "    showlegend=False # hide the trace from the legend\n",
    "))\n",
    "\n",
    "\n",
    "# create a new trace for the connecting line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=med_df.loc[5:9, \"PC1\"], # x-coordinates of the line\n",
    "    y=med_df.loc[5:9, \"PC2\"], # y-coordinates of the line\n",
    "    mode='lines', # specify the trace type as lines\n",
    "    line=dict(color=colour2, width=2), # set the color and width of the line\n",
    "    showlegend=False # hide the trace from the legend\n",
    "))\n",
    "'''\n",
    "# create a new trace for the connecting line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_score_mean.iloc[4:8, 0], # x-coordinates of the line\n",
    "    y=df_score_mean.iloc[4:8, 2], # y-coordinates of the line\n",
    "    mode='lines', # specify the trace type as lines\n",
    "    line=dict(color='#84CC56', width=2), # set the color and width of the line\n",
    "    showlegend=False # hide the trace from the legend\n",
    "))\n",
    "\n",
    "\n",
    "# create a new trace for the connecting line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_score_mean.iloc[8:12, 0], # x-coordinates of the line\n",
    "    y=df_score_mean.iloc[8:12, 2], # y-coordinates of the line\n",
    "    mode='lines', # specify the trace type as lines\n",
    "    line=dict(color='#CA83CC', width=2), # set the color and width of the line\n",
    "    showlegend=False # hide the trace from the legend\n",
    "))\n",
    "\n",
    "# create a new trace for the connecting line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_score_mean.iloc[8:12, 0], # x-coordinates of the line\n",
    "    y=df_score_mean.iloc[8:12, 2], # y-coordinates of the line\n",
    "    mode='lines', # specify the trace type as lines\n",
    "    line=dict(color='#6AE022', width=2), # set the color and width of the line\n",
    "    showlegend=False # hide the trace from the legend\n",
    "))\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                        #x=x_loc,\n",
    "                        x=1.0,\n",
    "                        y=0.05,\n",
    "                        showarrow=False,\n",
    "                        text='<b>R<sup>2</sup>X (Cum): {}%<b>'.format(np.round(df_explained_variance_.iloc[2,2]*100, decimals=2)),\n",
    "                        textangle=0,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"),\n",
    "                        # set alignment of text to left side of entry\n",
    "                        align=\"left\")\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                        #x=x_loc,\n",
    "                        x=1.0,\n",
    "                        y=0.01,\n",
    "                        showarrow=False,\n",
    "                        text='<b>Q<sup>2</sup>X (Cum): {}%<b>'.format(np.round(q2_test*100, decimals=2)),\n",
    "                        textangle=0,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"),\n",
    "                        # set alignment of text to left side of entry\n",
    "                        align=\"left\")\n",
    "\n",
    "fig.update_traces(marker=dict(size=20))\n",
    "#fig.update_traces(textposition='top center') #Text label position\n",
    "\n",
    "#fig.update_traces(marker=dict(size=12, color=Y1_color, marker=Y2_marker))\n",
    "fig.add_shape(type='path',\n",
    "            path=confidence_ellipse(med_df['PC1'],med_df['PC2']))\n",
    "\n",
    "\n",
    "\n",
    "#update axis as scitifics\n",
    "fig.update_xaxes(tickformat=\".1e\")\n",
    "fig.update_yaxes(tickformat=\".1e\")\n",
    "\n",
    "\n",
    "\n",
    "fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'},\n",
    "    font=dict(size=20))\n",
    "fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "#fig.show()\n",
    "\n",
    "fig.write_image(\"{}/Save_PCA_trajectory_\".format(Trajectory_save) + plot_name + \".png\")\n",
    "fig.write_html(\"{}/Save_PCA_trajectory_\".format(Trajectory_save) + plot_name + \".html\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "T2 = time.time()\n",
    "\n",
    "print('{} Done /n Time taken: {} seconds'.format(plot_name, T2-T1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/KOI_Xal.csv')\n",
    "ppm = pd.read_csv('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/KOI_ppm.csv')\n",
    "meta = pd.read_excel('/Volumes/CAS9/Aeiwz/Project/KOI_CC/Data/RunOrder_CKVremodel.xlsx', sheet_name='Data (Original)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.iloc[:, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfx.copy()\n",
    "ppm = list(np.ravel(ppm))\n",
    "df.columns = ppm\n",
    "dataset = pd.concat([meta, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =dataset.drop(dataset[dataset['Technique'] == 99].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Technique'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = dataset['Cardioplegia'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:, 10:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in project:\n",
    "    data= dataset[dataset['Cardioplegia'] == f]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # Import necessary libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.cross_decomposition import PLSRegression\n",
    "    from sklearn.model_selection import permutation_test_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.utils import shuffle\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.offline as pyo\n",
    "    import cross_validation\n",
    "    import plotting\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    from pyChemometrics import ChemometricsScaler\n",
    "\n",
    "    import os\n",
    "    from lingress.unipair import Unipair\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Make directory\n",
    "    # path folder\n",
    "    path_ = '/Volumes/CAS9/Aeiwz/Project/KOI_CC/Report/{}'.format(f)\n",
    "\n",
    "    os.makedirs(path_, exist_ok=True)\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs('{}/element'.format(path_), exist_ok=True)\n",
    "    os.makedirs('{}/element/hist_plot'.format(path_), exist_ok=True)\n",
    "    os.makedirs('{}/element/loading_plot'.format(path_), exist_ok=True)\n",
    "    os.makedirs('{}/element/score_plot'.format(path_), exist_ok=True)\n",
    "    os.makedirs('{}/element/s_plot'.format(path_), exist_ok=True)\n",
    "    os.makedirs('{}/main'.format(path_), exist_ok=True)\n",
    "    os.makedirs('{}/element/Lingress'.format(path_), exist_ok=True)\n",
    "\n",
    "    # Import the datasets from the /data directory\n",
    "\n",
    "\n",
    "    # X for the NMR spectra and Y for the 2 outcome variables\n",
    "    path_save = \"{}/element\".format(path_)\n",
    "    #test group\n",
    "    df = data\n",
    "    #Drop QC samples\n",
    "\n",
    "\n",
    "    Group=\"Time point\"\n",
    "\n",
    "    test_index = Unipair.indexing(meta = df, column_name=Group)\n",
    "\n",
    "\n",
    "\n",
    "    c_list = test_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(c_list)):\n",
    "        \n",
    "\n",
    "\n",
    "        test_select = df.copy()\n",
    "        test_select = test_select.loc[c_list[i]]\n",
    "        \n",
    "        \n",
    "        X = test_select.iloc[:, 119:]\n",
    "        df_X = X\n",
    "        #fill nan with 0\n",
    "        X = X.fillna(0)\n",
    "        meta = test_select.iloc[:, :119]\n",
    "        Y = test_select[Group]\n",
    "        Y1 = pd.Categorical(Y).codes\n",
    "        ppm = list(np.ravel(X.columns).astype(float))\n",
    "        # Use pandas Categorical type to generate the dummy enconding of the Y vector (0 and 1) \n",
    "        name = str(i)\n",
    "    \n",
    "\n",
    "        spectra = test_select.iloc[:, 119:]\n",
    "        ppm = list(spectra.columns.astype(float))\n",
    "        X = spectra.values\n",
    "        y = Y\n",
    "        \n",
    "        # Create a pipeline with data preprocessing and OPLS-DA model\n",
    "        pipeline = Pipeline([\n",
    "                                ('scale', ChemometricsScaler(scale_power=0.5)),\n",
    "                                ('oplsda', PLSRegression(n_components=2)),\n",
    "                                ('opls', cross_validation.CrossValidation(kfold=3, estimator='opls', scaler='pareto'))\n",
    "                            ])\n",
    "\n",
    "        oplsda = pipeline.named_steps['oplsda']\n",
    "        cv = pipeline.named_steps['opls']\n",
    "        cv.fit(X, y)\n",
    "\n",
    "        oplsda.fit(X, pd.Categorical(y).codes)\n",
    "        n_permutate = 1000\n",
    "\n",
    "        # Permutation test to assess the significance of the model\n",
    "        acc_score, permutation_scores, p_value = permutation_test_score(\n",
    "        pipeline.named_steps['oplsda'], X, pd.Categorical(y).codes, cv=3, n_permutations=n_permutate, n_jobs=-1, random_state=57, verbose=10)\n",
    "\n",
    "\n",
    "        s_scores_df = pd.DataFrame({'correlation': cv.correlation,'covariance': cv.covariance}, index=ppm)\n",
    "        df_opls_scores = pd.DataFrame({'t_scores': cv.scores, 't_ortho': cv.orthogonal_score, 't_pred': cv.predictive_score, 'label': y})\n",
    "\n",
    "            \n",
    "        colour_dict = {\n",
    "                \"Off pump CABG (OPCAB)\": \"#E91E63\",        \n",
    "                \"On pump CABG (ONCAB)\": \"#FF9800\",\n",
    "                \"custodiol\": \"#FFEB3B\",       \n",
    "                \"del Nido\": \"#9C27B0\",\n",
    "                \"st. thomas\": \"#03A9F4\",\n",
    "                \"No med\": \"#4CAF50\",        \n",
    "                \"7\": \"#B30000\",\n",
    "                \"8\": \"#3F51B5\"\n",
    "                }\n",
    "\n",
    "        #Visualise\n",
    "        from pca_ellipse import confidence_ellipse\n",
    "        fig = px.scatter(df_opls_scores, x='t_scores', y='t_ortho', \n",
    "                        color='label', \n",
    "                        color_discrete_map=colour_dict, \n",
    "                        title='<b>OPLS-DA Scores Plot<b>', \n",
    "                        height=900, width=1300,\n",
    "                        labels={\n",
    "                            't_pred': 't<sub>predict</sub>',\n",
    "                            't_ortho': 't<sub>orthogonal</sub>',\n",
    "                            't_scores': 't<sub>scores</sub>',\n",
    "                            'label': 'Intervention'}\n",
    "                        )\n",
    "\n",
    "        #fig.add_annotation(yref = 'paper', y = -1.06, xref = 'paper', x=1.06 , text='Q2' +' = {}'.format(np.round(df_explained_variance_.iloc[2,2], decimals=2)))\n",
    "        #fig.update_annotations(font = {\n",
    "        #    'size': 20}, showarrow=False)\n",
    "\n",
    "        #set data point fill alpha with boarder in each color\n",
    "        fig.update_traces(marker=dict(size=35, opacity=0.7, line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "        fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                                #x=x_loc,\n",
    "                                x=0,\n",
    "                                y=1.04+0.05,\n",
    "                                showarrow=False,\n",
    "                                text='<b>R<sup>2</sup>X: {}%<b>'.format(np.round(cv.R2Xcorr*100, decimals=2)),\n",
    "                                textangle=0,\n",
    "                                xref=\"paper\",\n",
    "                                yref=\"paper\"),\n",
    "                                # set alignment of text to left side of entry\n",
    "                                align=\"left\")\n",
    "\n",
    "        fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                                #x=x_loc,\n",
    "                                x=0,\n",
    "                                y=1.0+0.05,\n",
    "                                showarrow=False,\n",
    "                                text='<b>R<sup>2</sup>Y: {}%<b>'.format(np.round(cv.R2y*100, decimals=2)),\n",
    "                                textangle=0,\n",
    "                                xref=\"paper\",\n",
    "                                yref=\"paper\"),\n",
    "                                # set alignment of text to left side of entry\n",
    "                                align=\"left\")\n",
    "        fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                                #x=x_loc,\n",
    "                                x=0,\n",
    "                                y=1.08+0.05,\n",
    "                                showarrow=False,\n",
    "                                text='<b>Q<sup>2</sup>: {}%<b>'.format(np.round(cv.q2*100, decimals=2)),\n",
    "                                textangle=0,\n",
    "                                xref=\"paper\",\n",
    "                                yref=\"paper\"),\n",
    "                                # set alignment of text to left side of entry\n",
    "                                align=\"left\")\n",
    "\n",
    "        fig.add_shape(type='path',\n",
    "                path=confidence_ellipse(df_opls_scores['t_scores'], df_opls_scores['t_ortho']))\n",
    "\n",
    "\n",
    "        fig.update_traces(marker=dict(size=35))\n",
    "        #fig.update_traces(textposition='top center') #Text label position\n",
    "        #change M to 10^6\n",
    "        fig.update_yaxes(tickformat=\",.0\")\n",
    "        fig.update_xaxes(tickformat=\",.0\")\n",
    "\n",
    "        #fig.update_traces(marker=dict(size=12, color=Y1_color, marker=Y2_marker))\n",
    "\n",
    "        fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "        fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "        fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "        fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'y':1,\n",
    "                'x':0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'},\n",
    "            font=dict(size=20))\n",
    "        fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "\n",
    "        #fig.show()\n",
    "        fig.write_image(\"{}/score_plot/score_plot_{}.png\".format(path_save, name))\n",
    "        fig.write_html(\"{}/score_plot/score_plot{}.html\".format(path_save, name))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Histrogram\n",
    "        #Plot histogram of permutation scores\n",
    "        fig = px.histogram(permutation_scores, nbins=50, height=500, width=1000, \n",
    "                        title='<b>Permutation scores<b>',\n",
    "                        labels={'value': 'Accuracy score', \n",
    "                                'count': 'Frequency'})\n",
    "        #add dashed line to indicate the accuracy score of the model line y location is maximum count of histogram\n",
    "        fig.add_shape(type='line', yref='paper', y0=0, y1=1, xref='x', x0=acc_score, x1=acc_score, line=dict(dash='dash', color='red', width=3))\n",
    "\n",
    "\n",
    "        fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                                #x=x_loc,\n",
    "                                x=0,\n",
    "                                y=1.25,\n",
    "                                #y=1.18,\n",
    "                                showarrow=False,\n",
    "                                text='Number of permutation: {}'.format(n_permutate),\n",
    "                                textangle=0,\n",
    "                                xref=\"paper\",\n",
    "                                yref=\"paper\"),\n",
    "                                # set alignment of text to left side of entry\n",
    "                                align=\"left\")\n",
    "\n",
    "        fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                                #x=x_loc,\n",
    "                                x=0,\n",
    "                                y=1.18,\n",
    "                                showarrow=False,\n",
    "                                text='Accuracy score: {}'.format(np.round(acc_score, decimals=3)),\n",
    "                                textangle=0,\n",
    "                                xref=\"paper\",\n",
    "                                yref=\"paper\"),\n",
    "                                # set alignment of text to left side of entry\n",
    "                                align=\"left\")\n",
    "        fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                                #x=x_loc,\n",
    "                                x=0,\n",
    "                                y=1.11,\n",
    "                                showarrow=False,\n",
    "                                text='<i>p-value</i>: {}'.format(np.round(p_value, decimals=6)),\n",
    "                                textangle=0,\n",
    "                                xref=\"paper\",\n",
    "                                yref=\"paper\"),\n",
    "                                # set alignment of text to left side of entry\n",
    "                                align=\"left\")\n",
    "\n",
    "        fig.update_layout(showlegend=False)\n",
    "\n",
    "        fig.update_layout(title_x=0.5)\n",
    "\n",
    "        #fig.show()\n",
    "        fig.write_image(\"{}/hist_plot/Permutation_scores_{}.png\".format(path_save, name))\n",
    "        fig.write_html(\"{}/hist_plot/Permutation_scores_{}.html\".format(path_save, name))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #S plot\n",
    "        # sub-plot covariance for x and correlation for y S-plot using plotly, color by covariance with jet colormap\n",
    "        #setup figure size\n",
    "\n",
    "\n",
    "        fig = px.scatter(s_scores_df, x='covariance', y='correlation', color='covariance', range_color=[-1,1],\n",
    "                        color_continuous_scale='jet', text=s_scores_df.index, height=900, width=2000)\n",
    "        fig.update_layout(title='<b>S-plot</b>', xaxis_title='Covariance', yaxis_title='Correlation')\n",
    "\n",
    "        #add line of axis and set color to black and line width to 2 pixel\n",
    "        fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "        fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "        #Add tick width to 2 pixel\n",
    "        fig.update_xaxes(tickwidth=2)\n",
    "        fig.update_yaxes(tickwidth=2)\n",
    "        fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "        fig.update_yaxes(tickformat=\",.0\")\n",
    "        #fig.update_xaxes(tickformat=\",.0\")\n",
    "        fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "        fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "        fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "        fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'y':1,\n",
    "                'x':0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'},\n",
    "            font=dict(size=20))\n",
    "        #Set font size to 20\n",
    "        #Set marker size to 5 pixel\n",
    "        fig.update_traces(marker=dict(size=14))\n",
    "        #fig.show()\n",
    "        fig.write_image(\"{}/s_plot/S_plot_{}.png\".format(path_save, name))\n",
    "        fig.write_html(\"{}/s_plot/S_plot_{}.html\".format(path_save, name))\n",
    "        \n",
    "\n",
    "        #Loadings plot\n",
    "        \n",
    "        # X * 1 when correlation is positive, X * -1 when correlation is negative\n",
    "        def median_corr(X):\n",
    "            X_corr = np.median(X, axis=0)\n",
    "            X_corr = X_corr * np.sign(s_scores_df['correlation'])\n",
    "            return X_corr\n",
    "\n",
    "        X2 = median_corr(X)\n",
    "\n",
    "        fig = px.scatter(s_scores_df, x=ppm, y=X2, color='covariance', color_continuous_scale='jet', text=s_scores_df.index, height=500, width=2000)\n",
    "\n",
    "        fig.update_traces(marker=dict(size=3))\n",
    "        fig.update_xaxes(autorange=\"reversed\")\n",
    "        fig.update_layout(title='<b>Median spectra</b>', xaxis_title='ppm', yaxis_title='Correlation')\n",
    "        fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "        fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "        #Add tick width to 2 pixel\n",
    "        fig.update_xaxes(tickwidth=2)\n",
    "        fig.update_yaxes(tickwidth=2)\n",
    "\n",
    "        fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "        fig.update_yaxes(tickformat=\",.0\")\n",
    "        #fig.update_xaxes(tickformat=\",.0\")\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "                'y':1,\n",
    "                'x':0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'},\n",
    "            font=dict(size=20))\n",
    "        #Set marker size to 5 pixel\n",
    "        fig.update_traces(marker=dict(size=3))\n",
    "        #fig.show()\n",
    "        fig.write_image(\"{}/loading_plot/loadings_plot_{}.png\".format(path_save, name))\n",
    "        fig.write_html(\"{}/loading_plot/loadings_plot_{}.html\".format(path_save, name))\n",
    "        \n",
    "        from lingress.Lingress import lin_regression\n",
    "        lin_mod = lin_regression(x=df_X, target=meta[Group], label=meta[Group], features_name=ppm)\n",
    "        lin_mod.create_dataset()\n",
    "        lin_mod.fit_model(adj_method='fdr_bh')\n",
    "        report = lin_mod.report()\n",
    "        report.to_csv(\"{}/Lingress/lingress_report_{}.csv\".format(path_save, name))\n",
    "        lin_mod.volcano_plot()\n",
    "        lin_mod.png_plot(plot_name=\"lingress_volcano_plot_{}_{}\".format(f,name), path_save=path_save)\n",
    "        lin_mod.html_plot(plot_name=\"lingress_volcano_plot_{}_{}\".format(f,name), path_save=path_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate HTML file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "__author__ = \"aeiwz\"\n",
    "\n",
    "\n",
    "class gen_page:\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        '''\n",
    "        This function takes in the path to the data folder and returns the HTML files for the OPLS-DA plots.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_path: str\n",
    "            The path to the data folder.\n",
    "        gen_page(data_path).get_files()\n",
    "        '''\n",
    "        self.data_path = data_path\n",
    "\n",
    "        if data_path[-1] == '/':\n",
    "            #remove the last /\n",
    "            data_path = data_path[:-1]\n",
    "            \n",
    "        else:\n",
    "            data_path = data_path\n",
    "\n",
    "        \n",
    "\n",
    "        #check data_path is a string\n",
    "        if not isinstance(data_path, str):\n",
    "            raise ValueError(\"data_path should be a string\")\n",
    "        \n",
    "        #check data_path is a directory\n",
    "        if not os.path.isdir(data_path):\n",
    "            raise ValueError(\"data_path should be a directory\")\n",
    "\n",
    "        #check if data_path is empty\n",
    "        if not os.listdir(data_path):\n",
    "            raise ValueError(\"data_path should not be empty\")\n",
    "\n",
    "        #check if data_path contains the necessary files\n",
    "        if not os.path.exists(data_path+'/element/hist_plot'):\n",
    "            raise ValueError(\"data_path should contain a folder named 'element' with a folder named 'hist_plot'\")\n",
    "        if not os.path.exists(data_path+'/element/Lingress'):\n",
    "            raise ValueError(\"data_path should contain a folder named 'element' with a folder named 'Lingress'\")\n",
    "        if not os.path.exists(data_path+'/element/loading_plot'):\n",
    "            raise ValueError(\"data_path should contain a folder named 'element' with a folder named 'loading_plot'\")\n",
    "        if not os.path.exists(data_path+'/element/s_plot'):\n",
    "            raise ValueError(\"data_path should contain a folder named 'element' with a folder named 's_plot'\")\n",
    "        if not os.path.exists(data_path+'/element/score_plot'):\n",
    "            raise ValueError(\"data_path should contain a folder named 'element' with a folder named 'score_plot'\")\n",
    "\n",
    "\n",
    "\n",
    "        #change directory to data_path\n",
    "        os.chdir(data_path)\n",
    "\n",
    "    def get_files(self):\n",
    "        \n",
    "        data_path = self.data_path\n",
    "\n",
    "\n",
    "        hist_plot = glob(pathname= data_path+'/element/hist_plot/*.html')\n",
    "        Lingress_ = glob(pathname= data_path+'/element/Lingress/*.html')\n",
    "        loading_plot = glob(pathname= data_path+'/element/loading_plot/*.html')\n",
    "        s_plot = glob(pathname= data_path+'/element/s_plot/*.html')\n",
    "        score_plot = glob(pathname= data_path+'/element/score_plot/*.html')\n",
    "\n",
    "        files = pd.DataFrame({'hist_plot': hist_plot, 'Lingress': Lingress_, 'loading_plot': loading_plot, 's_plot': s_plot, 'score_plot': score_plot})\n",
    "\n",
    "        # Get the name of the files\n",
    "        files['names'] = files['hist_plot'].str.split('/').str[-1].str.split('.').str[0]\n",
    "        files['names'] = files['names'].str.replace('Permutation_scores_','')\n",
    "        files['names'] = files['names'].str.replace(' ','_')\n",
    "        #replace value in dataframe with ..\n",
    "        files = files.replace(to_replace=data_path, value='..', regex=True)\n",
    "\n",
    "        html_content_list = []\n",
    "\n",
    "        # Iterate through the name and file_ lists to create HTML files\n",
    "        for i in range(len(files)):\n",
    "            html_content = f\"\"\"\n",
    "\n",
    "        <!DOCTYPE html>\n",
    "        <html lang=\"en\">\n",
    "        <head>\n",
    "        <title>OPLS-DA</title>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "        <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
    "        <meta name=\"description\" content=\"HTML5 website template\">\n",
    "        <meta name=\"keywords\" content=\"CASIX, template, html, sass, jquery\">\n",
    "        <meta name=\"author\" content=\"CASIX\">\n",
    "\n",
    "        <link rel=\"ix-icon\" type=\"image/png\" href=\"assets/img/logo.PNG\">\n",
    "        <link rel=\"shortcut icon\" type=\"image/png\" href=\"assets/img/logo.PNG\">\n",
    "        </head>\n",
    "        <body>\n",
    "\n",
    "        <body>\n",
    "\n",
    "            <div class=\"container\">\n",
    "                <iframe src=\"{files['score_plot'][i]}\" \n",
    "                frameborder=\"0\" \n",
    "                width=\"100%\" \n",
    "                height=\"1000\"></iframe>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"container\">\n",
    "                <iframe src=\"{files['loading_plot'][i]}\" \n",
    "                frameborder=\"0\" \n",
    "                width=\"100%\" \n",
    "                height=\"1000\"></iframe>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"container\">\n",
    "                <iframe src=\"{files['s_plot'][i]}\" \n",
    "                frameborder=\"0\" \n",
    "                width=\"100%\" \n",
    "                height=\"600\"></iframe>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"container\">\n",
    "                <iframe src=\"{files['hist_plot'][i]}\" \n",
    "                frameborder=\"0\" \n",
    "                width=\"100%\" height=\"600\"></iframe>\n",
    "\n",
    "            </div>\n",
    "\n",
    "                <div class=\"container\">\n",
    "                <iframe src=\"{files['Lingress'][i]}\" \n",
    "                frameborder=\"0\" \n",
    "                width=\"100%\" height=\"600\"></iframe>\n",
    "            </div>\n",
    "\n",
    "\n",
    "        </body>\n",
    "        </html>\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            html_content_list.append(html_content)\n",
    "        \n",
    "        for i in range(len(files)):\n",
    "            # Write the HTML content to the file\n",
    "            file_path = f\"./main/oplsda_{files['names'][i]}.html\"\n",
    "            with open(file_path, \"w\") as html_file:\n",
    "                html_file.write(html_content_list[i])\n",
    "\n",
    "        return print('HTML files created')\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Volumes/CAS9/Aeiwz/Documents/Thesis/Analyse/Report/OPLSDA_U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgen_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 79\u001b[0m, in \u001b[0;36mgen_page.get_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m files \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhist_plot\u001b[39m\u001b[38;5;124m'\u001b[39m: hist_plot, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLingress\u001b[39m\u001b[38;5;124m'\u001b[39m: Lingress_, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading_plot\u001b[39m\u001b[38;5;124m'\u001b[39m: loading_plot, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms_plot\u001b[39m\u001b[38;5;124m'\u001b[39m: s_plot, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_plot\u001b[39m\u001b[38;5;124m'\u001b[39m: score_plot})\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Get the name of the files\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhist_plot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     80\u001b[0m files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermutation_scores_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m files[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[0;32m--> 182\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/strings/accessor.py:181\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical \u001b[38;5;241m=\u001b[39m is_categorical_dtype(data\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/strings/accessor.py:235\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    232\u001b[0m inferred_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .str accessor with string values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "gen_page(data_path=file_path).get_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('/Volumes/CAS9/Aeiwz/Documents/Thesis/Analyse/Dataset/U_noesy_pqn.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_ = ['sham ad libitum, 2-wk post-op', 'CR + INT777 (H), 2-wk post-op', 'CR + INT777 (L), 2-wk post-op', 'sham ad libitum, 1-wk post-op', 'CR + INT777 (H), 1-wk post-op', 'CR + INT777 (L), 2-wk post-op', 'CR + INT777 (H), 4-wk post-op', 'CR + INT777 (L), 1-wk post-op', 'CR + INT777 (L), 1-wk pre-op', 'CR + INT777 (L), 4-wk post-op', 'sham ad libitum, 1-wk post-op', 'sham ad libitum, 1-wk pre-op', 'CR + INT777 (H), 4-wk post-op', 'sham ad libitum, 1-wk pre-op', 'CR + INT777 (L), 4-wk post-op', 'sham ad libitum, 4-wk post-op', 'CR + INT777 (L), 1-wk pre-op', 'CR + INT777 (H), 1-wk pre-op', 'CR + INT777 (H), 4-wk post-op', 'sham ad libitum, 1-wk pre-op', 'CR + INT777 (H), 1-wk pre-op', 'sham ad libitum, 2-wk post-op', 'CR + INT777 (H), 4-wk post-op', 'CR + INT777 (L), 4-wk post-op', 'sham ad libitum, 4-wk post-op', 'CR + INT777 (H), 2-wk post-op', 'CR + INT777 (H), 4-wk post-op', 'CR + INT777 (L), 4-wk post-op', 'CR + INT777 (L), 4-wk post-op', 'sham ad libitum, 2-wk post-op', 'CR + INT777 (H), 2-wk post-op', 'sham ad libitum, 2-wk post-op', 'sham ad libitum, 1-wk post-op', 'CR + INT777 (H), 2-wk post-op', 'CR + INT777 (L), 1-wk post-op', 'CR + INT777 (L), 1-wk pre-op', 'sham ad libitum, 1-wk pre-op', 'CR + INT777 (H), 1-wk post-op', 'sham ad libitum, 1-wk pre-op', 'sham ad libitum, 2-wk post-op', 'CR + INT777 (L), 1-wk post-op', 'sham ad libitum, 4-wk post-op', 'CR + INT777 (L), 2-wk post-op', 'CR + INT777 (H), 1-wk post-op', 'CR + INT777 (H), 1-wk pre-op', 'sham ad libitum, 1-wk post-op', 'sham ad libitum, 4-wk post-op', 'sham ad libitum, 4-wk post-op', 'CR + INT777 (L), 2-wk post-op', 'CR + INT777 (L), 1-wk pre-op', 'CR + INT777 (L), 1-wk post-op', 'CR + INT777 (H), 1-wk pre-op', 'CR + INT777 (L), 1-wk pre-op', 'sham ad libitum, 1-wk post-op', 'CR + INT777 (L), 2-wk post-op', 'CR + INT777 (L), 1-wk post-op', 'CR + INT777 (H), 2-wk post-op', 'CR + INT777 (H), 1-wk pre-op', 'CR + INT777 (H), 1-wk post-op', 'CR + INT777 (H), 1-wk post-op']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'] = Class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1826172344592381s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.12208771705627441s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19360089302062988s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 400 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 536 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 672 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 824 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 955 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.3s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 2497494.94it/s]\n",
      "Features processed: 22767it [00:29, 778.88it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_CR + INT777 (H), 2-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0976099967956543s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.16067719459533691s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.17542314529418945s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 889 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.2s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4334031.64it/s]\n",
      "Features processed: 22767it [00:31, 724.16it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_CR + INT777 (L), 2-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.05982208251953125s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.09436583518981934s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.16278910636901855s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4625416.28it/s]\n",
      "Features processed: 22767it [00:29, 764.93it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_sham ad libitum, 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.03921627998352051s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1852217069228028s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1178140640258789s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 288 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 408 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 528 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 664 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 800 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 3984300.04it/s]\n",
      "Features processed: 22767it [00:28, 791.71it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_CR + INT777 (H), 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.05051589012145996s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19703646447998052s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.12127208709716797s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 336 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 456 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 576 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 712 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 951 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 984 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4709825.85it/s]\n",
      "Features processed: 22767it [00:28, 792.31it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_CR + INT777 (H), 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.052243947982788086s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1085500717163086s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.13190293312072754s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 889 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4748233.26it/s]\n",
      "Features processed: 22767it [00:30, 752.63it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_CR + INT777 (L), 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0564119815826416s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.11777496337890625s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.16276216506958008s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4813575.92it/s]\n",
      "Features processed: 22767it [00:30, 736.82it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_CR + INT777 (L), 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04474806785583496s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.12010312080383301s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.15441012382507324s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 889 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 950 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.2s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4488869.42it/s]\n",
      "Features processed: 22767it [00:29, 762.72it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_CR + INT777 (L), 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.07466864585876465s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.12662768363952637s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.13205504417419434s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.0s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 3347063.41it/s]\n",
      "Features processed: 22767it [00:40, 563.40it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_sham ad libitum, 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19693976509548683s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.10100913047790527s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1619720458984375s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 400 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 536 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 672 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 824 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 955 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.6s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4836738.04it/s]\n",
      "Features processed: 22767it [00:31, 723.78it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_sham ad libitum, 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1297318935394287s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.15317821502685547s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.17708992958068848s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 943 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.8s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4585656.89it/s]\n",
      "Features processed: 22767it [00:29, 765.10it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 2-wk post-op_vs_CR + INT777 (H), 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.049154043197631836s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.09028100967407227s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.18235516548156738s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4710058.16it/s]\n",
      "Features processed: 22767it [00:29, 780.38it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_CR + INT777 (L), 2-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.07385802268981934s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.18886423110961917s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1324770450592041s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 336 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 456 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 576 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 712 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 958 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 984 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4612458.06it/s]\n",
      "Features processed: 22767it [00:29, 784.88it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_sham ad libitum, 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.06607198715209961s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.11981201171875s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.14563608169555664s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 950 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    2.9s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4082412.86it/s]\n",
      "Features processed: 22767it [00:30, 758.71it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_CR + INT777 (H), 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.09029293060302734s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19257625579833987s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.16288518905639648s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 336 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 456 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 576 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 712 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 984 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.8s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4666555.21it/s]\n",
      "Features processed: 22767it [00:31, 727.99it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_CR + INT777 (H), 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04498577117919922s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.13011908531188965s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.14347100257873535s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.4s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4631248.81it/s]\n",
      "Features processed: 22767it [00:32, 706.47it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_CR + INT777 (L), 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0733339786529541s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19264738539123544s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.17128825187683105s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 336 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 456 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 576 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 712 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 984 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.5s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4407852.62it/s]\n",
      "Features processed: 22767it [00:31, 732.67it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_CR + INT777 (L), 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.047241926193237305s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19819339206343506s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19938027381896975s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 208 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 448 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 584 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 720 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 872 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 961 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.4s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4635970.44it/s]\n",
      "Features processed: 22767it [00:30, 739.72it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_CR + INT777 (L), 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0521540641784668s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.11673188209533691s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.16149091720581055s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.0s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4672034.79it/s]\n",
      "Features processed: 22767it [00:29, 766.37it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_sham ad libitum, 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.05663299560546875s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1983815687866211s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19750285148620605s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 336 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 456 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 576 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 712 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 848 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 951 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 984 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.4s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4805098.33it/s]\n",
      "Features processed: 22767it [00:32, 702.30it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_sham ad libitum, 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.04984879493713379s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.11998891830444336s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1766948699951172s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 943 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.2s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4524387.34it/s]\n",
      "Features processed: 22767it [00:30, 743.27it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (H), 2-wk post-op_vs_CR + INT777 (H), 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.12746906280517578s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.10467195510864258s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1981254035241312s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 906 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 953 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.4s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4406835.53it/s]\n",
      "Features processed: 22767it [00:32, 691.87it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_sham ad libitum, 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.10019898414611816s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.11629605293273926s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1278679370880127s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    2.6s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4658587.14it/s]\n",
      "Features processed: 22767it [00:37, 608.76it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_CR + INT777 (H), 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.11667799949645996s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.15098810195922852s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.17572426795959473s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    6.2s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4752959.99it/s]\n",
      "Features processed: 22767it [00:34, 662.28it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_CR + INT777 (H), 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0630807876586914s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.13989996910095215s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1655900478363037s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 889 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.4s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4774347.24it/s]\n",
      "Features processed: 22767it [00:31, 730.89it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_CR + INT777 (L), 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.05569314956665039s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.12797307968139648s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.18266606330871582s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.6s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4263213.50it/s]\n",
      "Features processed: 22767it [00:32, 699.93it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_CR + INT777 (L), 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.09175324440002441s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.11751079559326172s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19962630034392653s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 913 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 960 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.5s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4384980.45it/s]\n",
      "Features processed: 22767it [00:30, 752.46it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_CR + INT777 (L), 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.05710411071777344s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.13321495056152344s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.151749849319458s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4605117.63it/s]\n",
      "Features processed: 22767it [00:29, 759.15it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_sham ad libitum, 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.09719204902648926s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.15779876708984375s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1420590877532959s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.2s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4187498.65it/s]\n",
      "Features processed: 22767it [00:31, 726.61it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_sham ad libitum, 4-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0641021728515625s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1078488826751709s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1715068817138672s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 760 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 896 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 957 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 976 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.1s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 4307638.00it/s]\n",
      "Features processed: 22767it [00:36, 616.46it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish CR + INT777 (L), 2-wk post-op_vs_CR + INT777 (H), 1-wk pre-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.14345908164978027s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19934517919079425s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.19651234626770023s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 392 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 528 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 680 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 832 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 965 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    3.9s finished\n",
      "Creating data frame: 100%|██████████| 22767/22767 [00:00<00:00, 2227679.73it/s]\n",
      "Features processed: 22767it [00:39, 582.93it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjustment p-value with Benjamini/Hochberg (non-negative) Done\n",
      "Finish sham ad libitum, 1-wk post-op_vs_CR + INT777 (H), 1-wk post-op\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.07113504409790039s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.16689801216125488s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 308 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 436 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 580 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 656 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 740 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 824 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 916 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done 984 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.4s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.utils import shuffle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "import cross_validation\n",
    "import plotting\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyChemometrics import ChemometricsScaler\n",
    "\n",
    "import os\n",
    "from lingress.unipair import Unipair\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Make directory\n",
    "# path folder\n",
    "\n",
    "data = pd.read_csv('/Volumes/CAS9/Aeiwz/Documents/Thesis/Analyse/Dataset/U_noesy_pqn.csv')\n",
    "\n",
    "#exclude INT_H in index number 59\n",
    "data = data.drop([59])\n",
    "\n",
    "\n",
    "pair = Unipair(meta=data, column_name='Class')\n",
    "\n",
    "dataset = pair.get_dataset()\n",
    "\n",
    "names = pair.get_name()\n",
    "\n",
    "\n",
    "path_ = \"/Volumes/CAS9/Aeiwz/Documents/Thesis/Analyse/Report/OPLSDA_U\"\n",
    "os.makedirs(path_, exist_ok=True)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('{}/element'.format(path_), exist_ok=True)\n",
    "os.makedirs('{}/element/hist_plot'.format(path_), exist_ok=True)\n",
    "os.makedirs('{}/element/loading_plot'.format(path_), exist_ok=True)\n",
    "os.makedirs('{}/element/score_plot'.format(path_), exist_ok=True)\n",
    "os.makedirs('{}/element/s_plot'.format(path_), exist_ok=True)\n",
    "os.makedirs('{}/main'.format(path_), exist_ok=True)\n",
    "os.makedirs('{}/element/Lingress'.format(path_), exist_ok=True)\n",
    "\n",
    "# Import the datasets from the /data directory\n",
    "\n",
    "\n",
    "# X for the NMR spectra and Y for the 2 outcome variables\n",
    "path_save = \"{}/element\".format(path_)\n",
    "#test group\n",
    "df = data\n",
    "#Drop QC samples\n",
    "#df = df.drop(df[df['Technique'] == 'QC'].index)\n",
    "\n",
    "Group=\"Class\"\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "\n",
    "    plot_name = names[i]\n",
    "\n",
    "\n",
    "    test_select = dataset[i]\n",
    "\n",
    "    \n",
    "    \n",
    "    X = test_select.iloc[:, 14:]\n",
    "    df_X = X\n",
    "    #fill nan with 0\n",
    "    X = X.fillna(0)\n",
    "    meta = test_select.iloc[:, :14]\n",
    "    Y = test_select[Group]\n",
    "    Y1 = pd.Categorical(Y).codes\n",
    "    ppm = list(np.ravel(X.columns).astype(float))\n",
    "    # Use pandas Categorical type to generate the dummy enconding of the Y vector (0 and 1) \n",
    "    name = names[i]\n",
    "   \n",
    "\n",
    "    spectra = test_select.iloc[:, 14:]\n",
    "    ppm = list(spectra.columns.astype(float))\n",
    "    X = spectra.values\n",
    "    y = Y\n",
    "    \n",
    "    # Create a pipeline with data preprocessing and OPLS-DA model\n",
    "    pipeline = Pipeline([\n",
    "                            ('scale', ChemometricsScaler(scale_power=0.5)),\n",
    "                            ('oplsda', PLSRegression(n_components=2)),\n",
    "                            ('opls', cross_validation.CrossValidation(kfold=3, estimator='opls', scaler='pareto'))\n",
    "                         ])\n",
    "\n",
    "    oplsda = pipeline.named_steps['oplsda']\n",
    "    cv = pipeline.named_steps['opls']\n",
    "    cv.fit(X, y)\n",
    "\n",
    "    oplsda.fit(X, pd.Categorical(y).codes)\n",
    "    n_permutate = 1000\n",
    "\n",
    "    # Permutation test to assess the significance of the model\n",
    "    acc_score, permutation_scores, p_value = permutation_test_score(\n",
    "    pipeline.named_steps['oplsda'], X, pd.Categorical(y).codes, cv=3, n_permutations=n_permutate, n_jobs=-1, random_state=57, verbose=10)\n",
    "\n",
    "\n",
    "    s_scores_df = pd.DataFrame({'correlation': cv.correlation,'covariance': cv.covariance}, index=ppm)\n",
    "    df_opls_scores = pd.DataFrame({'t_scores': cv.scores, 't_ortho': cv.orthogonal_score, 't_pred': cv.predictive_score, 'label': y})\n",
    "\n",
    "        \n",
    "    colour_dict = {\n",
    "                    \"sham ad libitum\": \"#F55D4D\",        \n",
    "                    \"CR + INT777 (H)\": \"#58E6BE\",\n",
    "                    \"CR + INT777 (L)\": \"#4E8BF5\",       \n",
    "                    }\n",
    "    \n",
    "    symbol_dict = {'1-wk pre-op': 'circle',\n",
    "                    '1-wk post-op': 'diamond',\n",
    "                    '2-wk post-op': 'square',\n",
    "                    '4-wk post-op': 'triangle-up'\n",
    "                    }\n",
    "\n",
    "\n",
    "    #Visualise\n",
    "    from pca_ellipse import confidence_ellipse\n",
    "    fig = px.scatter(df_opls_scores, x='t_scores', y='t_ortho', symbol=meta['Time point'], \n",
    "                \n",
    "                    symbol_map=symbol_dict,\n",
    "                \n",
    "                    color='label', \n",
    "                    color_discrete_map=colour_dict, \n",
    "                    title='<b>OPLS-DA Scores Plot<b>', \n",
    "                    height=900, width=1300,\n",
    "                    labels={\n",
    "                        't_pred': 't<sub>predict</sub>',\n",
    "                        't_ortho': 't<sub>orthogonal</sub>',\n",
    "                        't_scores': 't<sub>scores</sub>',\n",
    "                        'label': 'Intervention'}\n",
    "                    )\n",
    "\n",
    "    #fig.add_annotation(yref = 'paper', y = -1.06, xref = 'paper', x=1.06 , text='Q2' +' = {}'.format(np.round(df_explained_variance_.iloc[2,2], decimals=2)))\n",
    "    #fig.update_annotations(font = {\n",
    "    #    'size': 20}, showarrow=False)\n",
    "\n",
    "    #set data point fill alpha with boarder in each color\n",
    "    fig.update_traces(marker=dict(size=35, opacity=0.7, line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                            #x=x_loc,\n",
    "                            x=0,\n",
    "                            y=1.04+0.05,\n",
    "                            showarrow=False,\n",
    "                            text='<b>R<sup>2</sup>X: {}%<b>'.format(np.round(cv.R2Xcorr*100, decimals=2)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                            #x=x_loc,\n",
    "                            x=0,\n",
    "                            y=1.0+0.05,\n",
    "                            showarrow=False,\n",
    "                            text='<b>R<sup>2</sup>Y: {}%<b>'.format(np.round(cv.R2y*100, decimals=2)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=20),\n",
    "                            #x=x_loc,\n",
    "                            x=0,\n",
    "                            y=1.08+0.05,\n",
    "                            showarrow=False,\n",
    "                            text='<b>Q<sup>2</sup>: {}%<b>'.format(np.round(cv.q2*100, decimals=2)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "\n",
    "    fig.add_shape(type='path',\n",
    "            path=confidence_ellipse(df_opls_scores['t_scores'], df_opls_scores['t_ortho']))\n",
    "\n",
    "\n",
    "    fig.update_traces(marker=dict(size=35))\n",
    "    #fig.update_traces(textposition='top center') #Text label position\n",
    "    #change M to 10^6\n",
    "    fig.update_yaxes(tickformat=\",.0\")\n",
    "    fig.update_xaxes(tickformat=\",.0\")\n",
    "\n",
    "    #fig.update_traces(marker=dict(size=12, color=Y1_color, marker=Y2_marker))\n",
    "\n",
    "    fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'y':1,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        font=dict(size=20))\n",
    "    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "\n",
    "    #fig.show()\n",
    "    fig.write_image(\"{}/score_plot/score_plot_{}.png\".format(path_save, name))\n",
    "    fig.write_html(\"{}/score_plot/score_plot{}.html\".format(path_save, name))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Histrogram\n",
    "    #Plot histogram of permutation scores\n",
    "    fig = px.histogram(permutation_scores, nbins=50, height=500, width=1000, \n",
    "                    title='<b>Permutation scores<b>',\n",
    "                    labels={'value': 'Accuracy score', \n",
    "                            'count': 'Frequency'})\n",
    "    #add dashed line to indicate the accuracy score of the model line y location is maximum count of histogram\n",
    "    fig.add_shape(type='line', yref='paper', y0=0, y1=1, xref='x', x0=acc_score, x1=acc_score, line=dict(dash='dash', color='red', width=3))\n",
    "\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                            #x=x_loc,\n",
    "                            x=0,\n",
    "                            y=1.25,\n",
    "                            #y=1.18,\n",
    "                            showarrow=False,\n",
    "                            text='Number of permutation: {}'.format(n_permutate),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                            #x=x_loc,\n",
    "                            x=0,\n",
    "                            y=1.18,\n",
    "                            showarrow=False,\n",
    "                            text='Accuracy score: {}'.format(np.round(acc_score, decimals=3)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "    fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                            #x=x_loc,\n",
    "                            x=0,\n",
    "                            y=1.11,\n",
    "                            showarrow=False,\n",
    "                            text='<i>p-value</i>: {}'.format(np.round(p_value, decimals=6)),\n",
    "                            textangle=0,\n",
    "                            xref=\"paper\",\n",
    "                            yref=\"paper\"),\n",
    "                            # set alignment of text to left side of entry\n",
    "                            align=\"left\")\n",
    "\n",
    "    fig.update_layout(showlegend=False)\n",
    "\n",
    "    fig.update_layout(title_x=0.5)\n",
    "\n",
    "    #fig.show()\n",
    "    fig.write_image(\"{}/hist_plot/Permutation_scores_{}.png\".format(path_save, name))\n",
    "    fig.write_html(\"{}/hist_plot/Permutation_scores_{}.html\".format(path_save, name))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #S plot\n",
    "    # sub-plot covariance for x and correlation for y S-plot using plotly, color by covariance with jet colormap\n",
    "    #setup figure size\n",
    "\n",
    "\n",
    "    fig = px.scatter(s_scores_df, x='covariance', y='correlation', color='covariance', range_color=[-1,1],\n",
    "                     color_continuous_scale='jet', text=s_scores_df.index, height=900, width=2000)\n",
    "    fig.update_layout(title='<b>S-plot</b>', xaxis_title='Covariance', yaxis_title='Correlation')\n",
    "\n",
    "    #add line of axis and set color to black and line width to 2 pixel\n",
    "    fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    #Add tick width to 2 pixel\n",
    "    fig.update_xaxes(tickwidth=2)\n",
    "    fig.update_yaxes(tickwidth=2)\n",
    "    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "    fig.update_yaxes(tickformat=\",.0\")\n",
    "    #fig.update_xaxes(tickformat=\",.0\")\n",
    "    fig.update_xaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='Black')\n",
    "    fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'y':1,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        font=dict(size=20))\n",
    "    #Set font size to 20\n",
    "    #Set marker size to 5 pixel\n",
    "    fig.update_traces(marker=dict(size=14))\n",
    "    #fig.show()\n",
    "    fig.write_image(\"{}/s_plot/S_plot_{}.png\".format(path_save, name))\n",
    "    fig.write_html(\"{}/s_plot/S_plot_{}.html\".format(path_save, name))\n",
    "    \n",
    "\n",
    "    #Loadings plot\n",
    "    \n",
    "    # X * 1 when correlation is positive, X * -1 when correlation is negative\n",
    "    def median_corr(X):\n",
    "        X_corr = np.median(X, axis=0)\n",
    "        X_corr = X_corr * np.sign(s_scores_df['correlation'])\n",
    "        return X_corr\n",
    "\n",
    "    X2 = median_corr(X)\n",
    "\n",
    "    fig = px.scatter(s_scores_df, x=ppm, y=X2, color='covariance', color_continuous_scale='jet', text=s_scores_df.index, height=500, width=2000)\n",
    "\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.update_xaxes(autorange=\"reversed\")\n",
    "    fig.update_layout(title='<b>Median spectra</b>', xaxis_title='ppm', yaxis_title='Correlation')\n",
    "    fig.update_xaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    fig.update_yaxes(showline=True, linewidth=2, linecolor='black')\n",
    "    #Add tick width to 2 pixel\n",
    "    fig.update_xaxes(tickwidth=2)\n",
    "    fig.update_yaxes(tickwidth=2)\n",
    "\n",
    "    fig.update_layout(paper_bgcolor='rgba(0,0,0,0)',plot_bgcolor='rgba(0,0,0,0)')\n",
    "    fig.update_yaxes(tickformat=\",.0\")\n",
    "    #fig.update_xaxes(tickformat=\",.0\")\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'y':1,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        font=dict(size=20))\n",
    "    #Set marker size to 5 pixel\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    #fig.show()\n",
    "    fig.write_image(\"{}/loading_plot/loadings_plot_{}.png\".format(path_save, name))\n",
    "    fig.write_html(\"{}/loading_plot/loadings_plot_{}.html\".format(path_save, name))\n",
    "    \n",
    "    from lingress.Lingress import lin_regression\n",
    "    lin_mod = lin_regression(x=df_X, target=meta[Group], label=meta[Group], features_name=ppm)\n",
    "    lin_mod.create_dataset()\n",
    "    lin_mod.fit_model(adj_method='fdr_bh')\n",
    "    report = lin_mod.report()\n",
    "    report.to_csv(\"{}/Lingress/lingress_report_{}.csv\".format(path_save, name))\n",
    "    lin_mod.volcano_plot()\n",
    "    lin_mod.png_plot(plot_name=\"lingress/volcano_plot_{}\".format(name), path_save=path_save)\n",
    "    lin_mod.html_plot(plot_name=\"lingress/volcano_plot_{}\".format(name), path_save=path_save)\n",
    "\n",
    "    del X, Y, Y1, ppm, spectra, meta, df_X, s_scores_df, df_opls_scores, cv, oplsda, pipeline, acc_score, permutation_scores, p_value, fig, lin_mod, report, n_permutate, plot_name, \n",
    "    \n",
    "    print(\"Finish {}\".format(name))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
